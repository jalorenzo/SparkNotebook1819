{
  "paragraphs": [
    {
      "text": "%md\n### Jobs, stages and tasks\n-   A Spark program defines a DAG connecting the different RDDs\n    -   Transformations create children RDDs from the parent RDDs\n\n-   Actions translate this DAG into an execution plan\n    -   The driver sends a *job* to compute all the RDDs involved in the action\n    -   A job comprises one or more *stages*\n    -   Each stage is associated to one or more RDDs from the DAG\n    -   The stages are processed in order, launching individual *tasks* to compute segments of the RDDs\n\n-   Pipelining: several RDDs can be computed in a same stage if they verify that:\n    -   The RDDs can be obtained from their parents without data movement (e.g. *map*), or\n    -   if any of the RDDs has been cached on memory or disk\n\n-   The [Spark web interface](http://localhost:4040 \"PySpark on localhost\") shows information about the stages and tasks (more information: `toDebugString()` method in the RDDs)",
      "user": "anonymous",
      "dateUpdated": "2018-10-09 15:13:14.595",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eJobs, stages and tasks\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eA Spark program defines a DAG connecting the different RDDs\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eTransformations create children RDDs from the parent RDDs\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eActions translate this DAG into an execution plan\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eThe driver sends a \u003cem\u003ejob\u003c/em\u003e to compute all the RDDs involved in the action\u003c/li\u003e\n      \u003cli\u003eA job comprises one or more \u003cem\u003estages\u003c/em\u003e\u003c/li\u003e\n      \u003cli\u003eEach stage is associated to one or more RDDs from the DAG\u003c/li\u003e\n      \u003cli\u003eThe stages are processed in order, launching individual \u003cem\u003etasks\u003c/em\u003e to compute segments of the RDDs\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003ePipelining: several RDDs can be computed in a same stage if they verify that:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eThe RDDs can be obtained from their parents without data movement (e.g. \u003cem\u003emap\u003c/em\u003e), or\u003c/li\u003e\n      \u003cli\u003eif any of the RDDs has been cached on memory or disk\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eThe \u003ca href\u003d\"http://localhost:4040\" title\u003d\"PySpark on localhost\"\u003eSpark web interface\u003c/a\u003e shows information about the stages and tasks (more information: \u003ccode\u003etoDebugString()\u003c/code\u003e method in the RDDs)\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256454458_1123681613",
      "id": "20170721-212620_1632692642",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-09 15:13:10.679",
      "dateFinished": "2018-10-09 15:13:10.935",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n// From the sequence files at apat63_99-seq obtain, for each country and year, the number of patents\nimport org.apache.hadoop.io.Text\n\nval prdd \u003d sc.sequenceFile(\"../data/apat63_99-seq\", classOf[Text], classOf[Text])\nprintln(\"Number of partitions of the RDD \"+ prdd.getNumPartitions)\n\n//Each register at apar63_99-seq has a set (country, patent, year)\nval prdd2 \u003d prdd.map(p \u003d\u003e (p._1+\"-\"+p._2.toString().split(\",\")(1), 1) ).reduceByKey(_+_)\n                \nval s \u003d prdd2.take(10)\n\nprintln(\"\\nDebugging information:\")\nprintln(prdd2.toDebugString)",
      "user": "anonymous",
      "dateUpdated": "2018-10-30 16:51:28.905",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1508256454459_1123296865",
      "id": "20170723-161930_1514654774",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 13:24:21.016",
      "dateFinished": "2018-10-03 13:25:21.389",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Accumulators\n\nAggregate values from the *worker nodes*, which are then sent to the *driver*\n\n-   Useful to count events\n\n-   Only the driver can access its value\n\n-   Accumulators used on RDDs transformations could be incorrect\n\n    -   If the RDD is recalculated, the accumulator can be updated\n\n    -   This problem does not happen with actions\n\n-   By default, accumulators are integers or floats\n-  \"Custom accumulators\" can be created using [`AccumulatorParam`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.AccumulatorParam)",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:03:55.580",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eAccumulators\u003c/h3\u003e\n\u003cp\u003eAggregate values from the \u003cem\u003eworker nodes\u003c/em\u003e, which are then sent to the \u003cem\u003edriver\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eUseful to count events\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eOnly the driver can access its value\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eAccumulators used on RDDs transformations could be incorrect\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003eIf the RDD is recalculated, the accumulator can be updated\u003c/p\u003e\u003c/li\u003e\n      \u003cli\u003e\n      \u003cp\u003eThis problem does not happen with actions\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eBy default, accumulators are integers or floats\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\u0026ldquo;Custom accumulators\u0026rdquo; can be created using \u003ca href\u003d\"https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.AccumulatorParam\"\u003e\u003ccode\u003eAccumulatorParam\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256454459_1123296865",
      "id": "20170331-180057_1836578886",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:03:45.167",
      "dateFinished": "2018-10-03 14:03:48.313",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom numpy.random import randint\n\nneven \u003d sc.accumulator(0)\n\ndef isEven(n):\n    global neven\n    if n%2 \u003d\u003d 0:\n        neven +\u003d 1\n\nrdd \u003d sc.parallelize(randint(100, size\u003d10000))\nrdd.foreach(isEven)\n\nprint(\"N even: %d\" % neven.value)",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 13:42:44.055",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1508256454459_1123296865",
      "id": "20170721-183051_1152536440",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 13:42:44.500",
      "dateFinished": "2018-10-03 13:44:31.517",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Broadcast variables\n\n-   By default, all shared variables (not RDDs) are sent to all executors\n\n    -   They are forwarded on each operation in which they appear\n\n-   Broadcast variables: Send, in an efficient way, read-only variables to the workers\n\n    -   They are sent only once\n",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:24:23.383",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eBroadcast variables\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eBy default, all shared variables (not RDDs) are sent to all executors\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003eThey are forwarded on each operation in which they appear\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eBroadcast variables: Send, in an efficient way, read-only variables to the workers\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003eThey are sent only once\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256454459_1123296865",
      "id": "20170721-183153_571912860",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:24:23.397",
      "dateFinished": "2018-10-03 14:24:23.755",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom operator import add\n\n# dicc is a broadcast variable\ndicc\u003dsc.broadcast({\"a\":\"alpha\",\"b\":\"beta\",\"c\":\"gamma\"})\n\nrdd\u003dsc.parallelize([(\"a\", 1),(\"b\", 3),(\"a\", -4),(\"c\", 0)])\nreduced_rdd \u003d rdd.reduceByKey(add).map(lambda (x,y): (dicc.value[x],y))\n\nprint(reduced_rdd.collect())\n",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:25:15.584",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1508256454459_1123296865",
      "id": "20170721-183656_606745752",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:25:16.157",
      "dateFinished": "2018-10-03 14:25:18.704",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Working at partition level\n\nA map operation is performed for each element in the RDD\n\n-   This implies redundant operations (e.g. open a connection to a DB)\n\n-   Its efficiency might be poor\n\nA `map` and `foreach` can be done once per partition:\n\n-   Methods `mapPartitions()`, `mapPartitionsWithIndex()` y `foreachPartition()`\n",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:30:22.557",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eWorking at partition level\u003c/h3\u003e\n\u003cp\u003eA map operation is performed for each element in the RDD\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eThis implies redundant operations (e.g. open a connection to a DB)\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eIts efficiency might be poor\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA \u003ccode\u003emap\u003c/code\u003e and \u003ccode\u003eforeach\u003c/code\u003e can be done once per partition:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eMethods \u003ccode\u003emapPartitions()\u003c/code\u003e, \u003ccode\u003emapPartitionsWithIndex()\u003c/code\u003e y \u003ccode\u003eforeachPartition()\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256454459_1123296865",
      "id": "20170721-183843_615906180",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:30:20.767",
      "dateFinished": "2018-10-03 14:30:20.948",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nnums \u003d sc.parallelize([1,2,3,4,5,6,7,8,9], 2)\nprint(nums.glom().collect())\n\ndef addAndCount(iterador):\n    addCount \u003d [0,0]\n    for i in iterador:\n        addCount[0] +\u003d i\n        addCount[1] +\u003d 1\n    return addCount\n    \nprint(nums.mapPartitions(addAndCount).glom().collect())",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:31:17.067",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1508256454459_1123296865",
      "id": "20170721-183941_197164903",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:31:17.591",
      "dateFinished": "2018-10-03 14:31:19.122",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndef addAndCountIndex(index, it):\n    return \"Particion \"+str(index), addAndCount(it)\n\nprint(nums.mapPartitionsWithIndex(addAndCountIndex).glom().collect())",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:39:39.898",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1508256454460_1121373120",
      "id": "20170721-184054_936692370",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:39:40.400",
      "dateFinished": "2018-10-03 14:39:40.717",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport os\nimport tempfile\n\ndef f(iterator):\n    tempfich, _ \u003d tempfile.mkstemp(dir\u003dtempdir)\n    for x in iterator: \n        os.write(tempfich, str(x)+\u0027\\t\u0027)\n    os.close(tempfich)\n        \ntempdir \u003d \"/tmp/foreachPartition\"\n\nif not os.path.exists(tempdir):\n    os.mkdir(tempdir)\n    nums.foreachPartition(f)",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:40:02.101",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1508256454460_1121373120",
      "id": "20170721-193112_1735244858",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:40:02.246",
      "dateFinished": "2018-10-03 14:40:02.306",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\nTEMP\u003d/tmp/foreachPartition\necho \"Files created\"\nls -l $TEMP\necho\necho \"Files content:\"\nfor f in $TEMP/*;do cat $f; echo; echo \"\u003d\u003d\u003d\u003d\"; done\nrm -rf $TEMP",
      "user": "anonymous",
      "dateUpdated": "2018-10-03 14:41:01.515",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1508256454460_1121373120",
      "id": "20170721-193920_1402264741",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-03 14:41:01.703",
      "dateFinished": "2018-10-03 14:41:04.237",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## ASSIGNMENT 1\n\nDevelop a PySpark script that, from the sequence files in `apat63_99-seq`, creates a key/value RDD. On this RDD, the key will be a country and the value a list of tuples. Each tuple is composed of a year and the number of patents of the country in that year. In addition, you must use the content of the `country_codes.txt` file (available at `../data/country_codes.txt`) as a broadcast variable to replace the country code by its name. Lastly, the RDD must be sorted by contry name and, for each country, the list of values will be sorted by year.\n\nRemember that each register of `apat63_99-seq` has a key/value pair `(country  patent,year)`, being both the key and the value of *org.apache.hadoop.io.Text* type.\n",
      "user": "anonymous",
      "dateUpdated": "2018-10-30 16:54:44.126",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eASSIGNMENT 1\u003c/h2\u003e\n\u003cp\u003eDevelop a PySpark script that, from the sequence files in \u003ccode\u003eapat63_99-seq\u003c/code\u003e, creates a key/value RDD. On this RDD, the key will be a country and the value a list of tuples. Each tuple is composed of a year and the number of patents of the country in that year. In addition, you must use the content of the \u003ccode\u003ecountry_codes.txt\u003c/code\u003e file (available at \u003ccode\u003e../data/country_codes.txt\u003c/code\u003e) as a broadcast variable to replace the country code by its name. Lastly, the RDD must be sorted by contry name and, for each country, the list of values will be sorted by year.\u003c/p\u003e\n\u003cp\u003eRemember that each register of \u003ccode\u003eapat63_99-seq\u003c/code\u003e has a key/value pair \u003ccode\u003e(country  patent,year)\u003c/code\u003e, being both the key and the value of \u003cem\u003eorg.apache.hadoop.io.Text\u003c/em\u003e type.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256454460_1121373120",
      "id": "20170721-194333_894569065",
      "dateCreated": "2017-10-17 16:07:34.000",
      "dateStarted": "2018-10-30 16:54:42.939",
      "dateFinished": "2018-10-30 16:54:42.978",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "dateUpdated": "2017-10-17 16:07:34.000",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1508256454460_1121373120",
      "id": "20170723-115336_1788563352",
      "dateCreated": "2017-10-17 16:07:34.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark course/09 - Advanced concepts",
  "id": "2CX1QPZDM",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "sh:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}